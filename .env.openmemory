# Server Settings
OM_PORT=8080
OM_API_KEY=claude-code-satisfactory-tracker # NOTE: not an API key, just project name for running locally
OM_MODE=standard # standard | langgraph

# Database
OM_DB_PATH=./data/openmemory.sqlite

# Embeddings Configuration
# Available providers: openai, gemini, ollama, local, synthetic
OM_EMBEDDINGS=ollama
OM_VEC_DIM=768

# Embedding Mode
# simple   = 1 unified batch call for all sectors (faster, rate-limit safe, recommended)
# advanced = 5 separate calls, one per sector (higher precision, more API calls)
OM_EMBED_MODE=simple

# Advanced Mode Options (only used when OM_EMBED_MODE=advanced)
# Enable parallel embedding (not recommended for Gemini due to rate limits)
OM_ADV_EMBED_PARALLEL=false
# Delay between embeddings in milliseconds
OM_EMBED_DELAY_MS=200

# OpenAI Embeddings
OPENAI_API_KEY=

# Google Gemini Embeddings
GEMINI_API_KEY=

# Ollama Local Embeddings
OLLAMA_URL=http://localhost:11434

# Local Model Path (for custom embedding models)
LOCAL_MODEL_PATH=/path/to/your/local/model

# Memory System Settings
OM_MIN_SCORE=0.3
OM_DECAY_LAMBDA=0.02

# Brain Sector Configuration (auto-classified, but you can override)
# Sectors: episodic, semantic, procedural, emotional, reflective
# Using nomic-embed-text for all sectors (simpler, works great for technical content)
# Traditionally would use small model for procedural content, large model for reflective
OLLAMA_MODEL_EPISODIC=nomic-embed-text
OLLAMA_MODEL_SEMANTIC=nomic-embed-text
OLLAMA_MODEL_PROCEDURAL=nomic-embed-text
OLLAMA_MODEL_EMOTIONAL=nomic-embed-text
OLLAMA_MODEL_REFLECTIVE=nomic-embed-text

# LangGraph Integration Mode (LGM)
OM_LG_NAMESPACE=default
OM_LG_MAX_CONTEXT=50
OM_LG_REFLECTIVE=true
